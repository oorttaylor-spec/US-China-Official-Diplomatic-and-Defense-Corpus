# ==============================================================================
# US State Department (2021â€“2025) Press Releases Crawler
#
# Description (English)
# ------------------------------------------------------------------------------
# This script scrapes press releases from the U.S. State Department website:
#   https://2021-2025.state.gov/press-releases/
#
# It collects article metadata from index pages (publish time, type, title, URL),
# and then visits each article page to extract the full content (including <p>, <h3>,
# and nested <ul>/<li> lists with indentation and numbering).
#
# Workflow:
#   1) Load progress from CSV (resume support by source_url)
#   2) Collect article info from index pages (TOTAL_PAGES)
#   3) De-duplicate and filter out processed URLs
#   4) Crawl new article pages and append results to:
#        - CSV  (UTF-8 with BOM)  [progress/resume file]
#        - JSON (UTF-8, one JSON object per line)
#
# Resume / checkpoint logic:
#   - The script uses the CSV file as the progress record.
#   - If a source_url already exists in the CSV, it will be skipped on rerun.
#
# Output schema (per record)
# ------------------------------------------------------------------------------
#   publish_time, type, title, content, source_url
#
# Dependencies
# ------------------------------------------------------------------------------
#   pip install requests beautifulsoup4 pandas tqdm openpyxl urllib3 playwright
#
# ------------------------------------------------------------------------------
# ä¸­æ–‡è¯´æ˜
# ------------------------------------------------------------------------------
# æœ¬è„šæœ¬ç”¨äºçˆ¬å–ç¾å›½å›½åŠ¡é™¢ï¼ˆ2021-2025ï¼‰ç½‘ç«™çš„æ–°é—»ç®€æŠ¥ï¼š
#   https://2021-2025.state.gov/press-releases/
#
# ä»ç´¢å¼•é¡µæ”¶é›†æ–‡ç« ä¿¡æ¯ï¼ˆå‘å¸ƒæ—¶é—´ã€ç±»å‹ã€æ ‡é¢˜ã€é“¾æ¥ï¼‰ï¼Œå¹¶è¿›å…¥è¯¦æƒ…é¡µæå–æ­£æ–‡
# ï¼ˆåŒ…æ‹¬ pã€h3 ä»¥åŠ ul/li åµŒå¥—åˆ—è¡¨ï¼Œå¸¦ç¼©è¿›å’Œç¼–å·æ ¼å¼åŒ–ï¼‰ã€‚
#
# æµç¨‹ï¼š
#   1ï¼‰ä» CSV è¯»å–è¿›åº¦ï¼ˆæŒ‰ source_url æ–­ç‚¹ç»­çˆ¬ï¼‰
#   2ï¼‰ä»å¤šä¸ªç´¢å¼•é¡µï¼ˆTOTAL_PAGESï¼‰æ”¶é›†æ–‡ç« ä¿¡æ¯
#   3ï¼‰å»é‡å¹¶è¿‡æ»¤å·²çˆ¬å–é“¾æ¥
#   4ï¼‰çˆ¬å–æ–°æ–‡ç« è¯¦æƒ…å¹¶è¿½åŠ å†™å…¥ï¼š
#        - CSVï¼ˆUTF-8 å¸¦ BOMï¼‰ã€åŒæ—¶ä½œä¸ºæ–­ç‚¹ç»­çˆ¬è¿›åº¦æ–‡ä»¶ã€‘
#        - JSONLï¼ˆUTF-8ï¼Œæ¯è¡Œä¸€æ¡ JSON è®°å½•ï¼‰
#
# æ–­ç‚¹ç»­çˆ¬é€»è¾‘ï¼š
#   - ä»ç„¶ä»¥ CSV æ–‡ä»¶ä½œä¸ºè¿›åº¦è®°å½•
#   - è‹¥ CSV ä¸­å·²æœ‰æŸä¸ª source_urlï¼Œåˆ™ä¸‹æ¬¡è¿è¡Œä¼šè·³è¿‡è¯¥é“¾æ¥
#
# ä¾èµ–å®‰è£…ï¼š
#   pip install requests beautifulsoup4 pandas tqdm openpyxl urllib3 playwright
# ==============================================================================

import asyncio
import csv
import json
import os
import re
from typing import List, Dict

import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from tqdm import tqdm


# ---------------- å‚æ•° ----------------
BASE = "https://2021-2025.state.gov"
INDEX_BASE = f"{BASE}/press-releases/"
TOTAL_PAGES = 1111

# è¾“å‡ºæ–‡ä»¶ï¼ˆå»ºè®®æ”¾åœ¨åŒç›®å½•ï¼‰
OUT_CSV = "state_department_press_releases_2021_2025.csv"
OUT_JSON = "state_department_press_releases_2021_2025.json"


# ---------------- å·¥å…·å‡½æ•° ----------------
def ensure_dir_for(path: str):
    d = os.path.dirname(os.path.abspath(path))
    if d and not os.path.exists(d):
        os.makedirs(d, exist_ok=True)


def sanitize_text(txt: str) -> str:
    if not txt:
        return ""
    txt = txt.replace("\xa0", " ").replace("\r", " ")
    txt = re.sub(r"[ \t\f\v]+", " ", txt)
    return txt.strip()


def load_done_urls() -> set:
    if not os.path.exists(OUT_CSV):
        return set()
    try:
        df = pd.read_csv(OUT_CSV)
        return set(df["url"].dropna().tolist())
    except Exception:
        return set()


def open_writer():
    """è¿”å› (file_handle, writer)ï¼ŒCSVä¸ºUTF-8-SIG + QUOTE_ALL"""
    file_exists = os.path.exists(OUT_CSV)
    fh = open(OUT_CSV, "a", encoding="utf-8-sig", newline="")
    writer = csv.DictWriter(
        fh,
        fieldnames=["type", "title", "date", "url", "content"],
        quoting=csv.QUOTE_ALL,
        delimiter=",",
        lineterminator="\n",
        escapechar="\\",
    )
    if not file_exists:
        writer.writeheader()
    return fh, writer


def parse_index_items(html: str) -> List[Dict]:
    """è§£æç´¢å¼•é¡µï¼šæå–ç±»å‹(type)ã€æ ‡é¢˜(title)ã€æ—¥æœŸ(date)ã€URL(url)"""
    soup = BeautifulSoup(html, "lxml")
    items = []
    for li in soup.select("ul.collection-results li.collection-result"):
        a = li.select_one("a.collection-result__link[href]")
        if not a:
            continue
        href = a.get("href", "").strip()
        if href.startswith("/"):
            href = BASE + href
        title = sanitize_text(a.get_text(" ", strip=True))
        typ = sanitize_text(
            (li.select_one("p.collection-result__date") or {}).get_text(" ", strip=True)
        )
        date_el = li.select_one("div.collection-result-meta span:last-child")
        date_text = sanitize_text(
            date_el.get_text(" ", strip=True) if date_el else ""
        )
        if href and title:
            items.append(
                {"type": typ, "title": title, "url": href, "date": date_text}
            )
    return items


def parse_detail_content(html: str) -> str:
    """åªæå–æ­£æ–‡ï¼ˆä¿ç•™è‡ªç„¶æ®µï¼‰"""
    soup = BeautifulSoup(html, "lxml")
    entry = soup.select_one("div.entry-content") or soup.select_one("article")
    if not entry:
        return ""

    # åˆ é™¤ä¸ç›¸å…³å…ƒç´ 
    for bad in entry.select(
        "script, style, noscript, figure, iframe, form, input, button, svg"
    ):
        bad.decompose()
    for bad_sel in [
        ".post_tags",
        ".tags",
        ".report__back-to-top",
        ".page-header__actions",
        ".sharethis-inline-share-buttons",
        ".social-share",
        ".wp-block-buttons",
    ]:
        for b in entry.select(bad_sel):
            b.decompose()

    # æå–æ®µè½
    paragraphs = []
    for tag in entry.find_all(["h2", "h3", "p", "li", "blockquote"]):
        text = sanitize_text(tag.get_text(" ", strip=True))
        if text:
            paragraphs.append(text)

    if not paragraphs:
        text = entry.get_text("\n", strip=True)
        return sanitize_text(text)

    return "\n\n".join(paragraphs)


async def fetch_html(page, url: str, wait_selector: str = None, extra_wait_sec: float = 0.0) -> str:
    """åŠ è½½é¡µé¢HTML"""
    await page.goto(url, timeout=120_000, wait_until="domcontentloaded")
    if extra_wait_sec:
        await asyncio.sleep(extra_wait_sec)
    if wait_selector:
        await page.wait_for_selector(wait_selector, timeout=30_000)
    return await page.content()


def write_dual(data: Dict, writer, fh):
    """å†™å…¥CSVä¸JSONï¼Œä¿è¯æ­£æ–‡åœ¨CSVå•æ ¼å†…"""
    # å…ˆå¤åˆ¶å¹¶è½¬ä¹‰æ¢è¡Œç¬¦ï¼Œé˜²æ­¢Excelé”™è¡Œ
    csv_data = data.copy()
    if isinstance(csv_data["content"], str):
        csv_data["content"] = csv_data["content"].replace("\n", "\\n").replace("\r", "")
    writer.writerow(csv_data)
    fh.flush()

    # å†å†™JSONï¼ˆä¿ç•™åŸå§‹æ®µè½æ¢è¡Œï¼‰
    with open(OUT_JSON, "a", encoding="utf-8") as jf:
        jf.write(json.dumps(data, ensure_ascii=False) + "\n")


# ---------------- ä¸»çˆ¬è™« ----------------
async def crawl_all():
    ensure_dir_for(OUT_CSV)
    done = load_done_urls()
    print(f"âœ… å·²å®Œæˆ {len(done)} æ¡ï¼Œå¼€å§‹çˆ¬å–åˆ° {OUT_CSV} / {OUT_JSON}")

    fh, writer = open_writer()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122 Safari/537.36"
            )
        )
        page = await context.new_page()

        for i in tqdm(range(1, TOTAL_PAGES + 1), desc="ç´¢å¼•é¡µ"):
            index_url = INDEX_BASE if i == 1 else f"{INDEX_BASE}page/{i}/"
            try:
                index_html = await fetch_html(
                    page,
                    index_url,
                    wait_selector="ul.collection-results li.collection-result",
                    extra_wait_sec=2.0,
                )
            except Exception as e:
                print(f"âš ï¸ ç¬¬{i}é¡µåŠ è½½å¤±è´¥: {e}")
                continue

            items = parse_index_items(index_html)
            if not items:
                print(f"âš ï¸ ç¬¬{i}é¡µä¸ºç©ºæˆ–ç»“æ„å˜åŒ–ï¼š{index_url}")
                continue

            # æŠ“è¯¦æƒ…é¡µ
            for it in items:
                if it["url"] in done:
                    continue

                dpage = await context.new_page()
                try:
                    detail_html = await fetch_html(
                        dpage,
                        it["url"],
                        wait_selector="div.entry-content, article",
                        extra_wait_sec=1.5,
                    )
                    content = parse_detail_content(detail_html)
                except Exception as e:
                    print(f"âš ï¸ è¯¦æƒ…é¡µå¤±è´¥ {it['url']}: {e}")
                    content = ""
                finally:
                    await dpage.close()

                row = {
                    "type": it["type"],
                    "title": it["title"],
                    "date": it["date"],
                    "url": it["url"],
                    "content": content,
                }
                write_dual(row, writer, fh)
                done.add(it["url"])
                await asyncio.sleep(1.2)

        await browser.close()
    fh.close()
    print("ğŸ å…¨éƒ¨å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(crawl_all())
