# -*- coding: utf-8 -*-
# ==============================================================================
# US State Department Press Briefings Crawler (2021â€“2025)
#
# Description (English)
# ------------------------------------------------------------------------------
# This script scrapes press briefings from the U.S. State Department websites:
#   - Current version: https://www.state.gov/department-press-briefings/
#   - Archived version: https://2021-2025.state.gov/department-press-briefings/
#
# The script works by:
#   1) Collecting article metadata (publish time, type, title, URL) from index pages
#   2) Visiting each article page to extract the full content (including <p>, <h3>,
#      and nested <ul>/<li> lists with indentation and numbering).
#
# Workflow:
#   1) Load progress from CSV (to support resume by source_url)
#   2) Collect article info from index pages (TOTAL_PAGES)
#   3) De-duplicate and filter out processed URLs
#   4) Crawl new article pages and append results to:
#        - CSV  (UTF-8 with BOM) [progress/resume file]
#        - JSON (UTF-8, one JSON object per line)
#
# Resume / checkpoint logic:
#   - The script uses the CSV file as the progress record.
#   - If a source_url already exists in the CSV, it will be skipped on rerun.
#
# Output schema (per record):
# ------------------------------------------------------------------------------
#   source, type, title, date, url, content
#
# Dependencies
# ------------------------------------------------------------------------------
#   pip install requests beautifulsoup4 pandas tqdm openpyxl urllib3 playwright
#
# ------------------------------------------------------------------------------
# ä¸­æ–‡è¯´æ˜
# ------------------------------------------------------------------------------
# æœ¬è„šæœ¬ç”¨äºçˆ¬å–ç¾å›½å›½åŠ¡é™¢ç½‘ç«™çš„æ–°é—»ç®€æŠ¥ï¼ŒåŒ…æ‹¬ï¼š
#   - å½“å‰ç‰ˆæœ¬ï¼šhttps://www.state.gov/department-press-briefings/
#   - å½’æ¡£ç‰ˆæœ¬ï¼šhttps://2021-2025.state.gov/department-press-briefings/
#
# è„šæœ¬çš„åŠŸèƒ½ï¼š
#   1) ä»ç´¢å¼•é¡µæ”¶é›†æ–‡ç« å…ƒæ•°æ®ï¼ˆå‘å¸ƒæ—¶é—´ã€ç±»å‹ã€æ ‡é¢˜ã€é“¾æ¥ï¼‰
#   2) è®¿é—®æ¯ç¯‡æ–‡ç« çš„é¡µé¢å¹¶æå–å®Œæ•´å†…å®¹ï¼ˆåŒ…æ‹¬ <p>ã€<h3> æ ‡ç­¾ï¼Œ
#      ä»¥åŠå¸¦æœ‰ç¼©è¿›å’Œç¼–å·çš„åµŒå¥— <ul>/<li> åˆ—è¡¨ï¼‰
#
# æµç¨‹ï¼š
#   1ï¼‰ä» CSV è¯»å–è¿›åº¦ï¼ˆæŒ‰ source_url æ–­ç‚¹ç»­çˆ¬ï¼‰
#   2ï¼‰ä»å¤šä¸ªç´¢å¼•é¡µï¼ˆTOTAL_PAGESï¼‰æ”¶é›†æ–‡ç« ä¿¡æ¯
#   3ï¼‰å»é‡å¹¶è¿‡æ»¤å·²çˆ¬å–é“¾æ¥
#   4ï¼‰çˆ¬å–æ–°æ–‡ç« è¯¦æƒ…å¹¶è¿½åŠ å†™å…¥ï¼š
#        - CSVï¼ˆUTF-8 å¸¦ BOMï¼‰ã€ä½œä¸ºè¿›åº¦æ–‡ä»¶ã€‘
#        - JSONLï¼ˆUTF-8ï¼Œæ¯è¡Œä¸€ä¸ª JSON è®°å½•ï¼‰
#
# æ–­ç‚¹ç»­çˆ¬é€»è¾‘ï¼š
#   - è„šæœ¬ä½¿ç”¨ CSV æ–‡ä»¶è®°å½•è¿›åº¦ã€‚
#   - å¦‚æœ CSV ä¸­å·²å­˜åœ¨æŸä¸ª source_urlï¼Œåˆ™ä¸‹æ¬¡è¿è¡Œæ—¶è·³è¿‡è¯¥é“¾æ¥ã€‚
#
# è¾“å‡ºæ ¼å¼ï¼ˆæ¯æ¡è®°å½•ï¼‰ï¼š
# ------------------------------------------------------------------------------
#   source, type, title, date, url, content
#
# ä¾èµ–å®‰è£…ï¼š
#   pip install requests beautifulsoup4 pandas tqdm openpyxl urllib3 playwright
# ==============================================================================


import asyncio
import csv
import json
import os
import re
from typing import List, Dict

import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from tqdm import tqdm


# ---------------- å‚æ•° ----------------
SITES = [
    {"base": "https://www.state.gov", "index": "https://www.state.gov/department-press-briefings/", "pages": 7},
    {"base": "https://2021-2025.state.gov", "index": "https://2021-2025.state.gov/department-press-briefings/", "pages": 105},
]

OUT_CSV = "state_department_press_briefings.csv"
OUT_JSON = "state_department_press_briefings.json"


# ---------------- å·¥å…·å‡½æ•° ----------------
def ensure_dir_for(path: str):
    d = os.path.dirname(os.path.abspath(path))
    if d and not os.path.exists(d):
        os.makedirs(d, exist_ok=True)


def sanitize_text(txt: str) -> str:
    if not txt:
        return ""
    txt = txt.replace("\xa0", " ").replace("\r", " ")
    txt = re.sub(r"[ \t\f\v]+", " ", txt)
    return txt.strip()


def load_done_urls() -> set:
    if not os.path.exists(OUT_CSV):
        return set()
    try:
        df = pd.read_csv(OUT_CSV)
        return set(df["url"].dropna().tolist())
    except Exception:
        return set()


def open_writer():
    file_exists = os.path.exists(OUT_CSV)
    fh = open(OUT_CSV, "a", encoding="utf-8-sig", newline="")
    writer = csv.DictWriter(
        fh,
        fieldnames=["source", "type", "title", "date", "url", "content"],
        quoting=csv.QUOTE_ALL,
        delimiter=",",
        lineterminator="\n",
        escapechar="\\",
    )
    if not file_exists:
        writer.writeheader()
    return fh, writer


# ---------------- è§£æå‡½æ•° ----------------
def parse_index_items(html: str, base_url: str) -> List[Dict]:
    """æ–°ç‰ˆä¸æ—§ç‰ˆç´¢å¼•é¡µç»“æ„ç›¸åŒ"""
    soup = BeautifulSoup(html, "lxml")
    items = []
    for li in soup.select("li.collection-result"):
        a = li.select_one("a.collection-result__link[href]")
        if not a:
            continue
        href = a.get("href", "").strip()
        if href.startswith("/"):
            href = base_url + href
        title = sanitize_text(a.get_text(" ", strip=True))
        typ = sanitize_text(
            (li.select_one("p.collection-result__date") or {}).get_text(" ", strip=True)
        )
        date_el = li.select_one("div.collection-result-meta span:last-child")
        date_text = sanitize_text(date_el.get_text(" ", strip=True) if date_el else "")
        if href and title:
            items.append(
                {"type": typ, "title": title, "url": href, "date": date_text}
            )
    return items


def parse_detail_content(html: str) -> str:
    """æå–æ–°é—»ç®€æŠ¥æ­£æ–‡"""
    soup = BeautifulSoup(html, "lxml")
    entry = (
        soup.select_one("div.classic-block-wrapper")
        or soup.select_one("div.entry-content")
        or soup.select_one("article")
    )
    if not entry:
        return ""

    # åˆ é™¤ä¸ç›¸å…³å…ƒç´ 
    for bad in entry.select("script, style, noscript, figure, iframe, svg, form, input, button"):
        bad.decompose()

    paragraphs = []
    for tag in entry.find_all(["h2", "h3", "p", "li", "blockquote"]):
        text = sanitize_text(tag.get_text(" ", strip=True))
        if text:
            paragraphs.append(text)

    if not paragraphs:
        text = entry.get_text("\n", strip=True)
        return sanitize_text(text)

    return "\n\n".join(paragraphs)


# ---------------- ç½‘ç»œä¸å†™å…¥ ----------------
async def fetch_html(page, url: str, wait_selector: str = None, extra_wait_sec: float = 0.0) -> str:
    await page.goto(url, timeout=120_000, wait_until="domcontentloaded")
    if extra_wait_sec:
        await asyncio.sleep(extra_wait_sec)
    if wait_selector:
        await page.wait_for_selector(wait_selector, timeout=30_000)
    return await page.content()


def write_dual(data: Dict, writer, fh):
    csv_data = data.copy()
    if isinstance(csv_data["content"], str):
        csv_data["content"] = csv_data["content"].replace("\n", "\\n").replace("\r", "")
    writer.writerow(csv_data)
    fh.flush()
    with open(OUT_JSON, "a", encoding="utf-8") as jf:
        jf.write(json.dumps(data, ensure_ascii=False) + "\n")


# ---------------- ä¸»å‡½æ•° ----------------
async def crawl_all():
    ensure_dir_for(OUT_CSV)
    done = load_done_urls()
    print(f"âœ… å·²å®Œæˆ {len(done)} æ¡ï¼Œå¼€å§‹çˆ¬å–æ–°ç‰ˆ + æ—§ç‰ˆæ–°é—»ç®€æŠ¥ä¼š ...")

    fh, writer = open_writer()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/124 Safari/537.36"
        )

        for site in SITES:
            base, index_base, total_pages = site["base"], site["index"], site["pages"]
            print(f"\nğŸŒ å¼€å§‹çˆ¬å–: {base} å…± {total_pages} é¡µ")

            page = await context.new_page()
            for i in tqdm(range(1, total_pages + 1), desc=f"{base} ç´¢å¼•é¡µ"):
                index_url = index_base if i == 1 else f"{index_base}page/{i}/"
                try:
                    index_html = await fetch_html(page, index_url, "li.collection-result", 2.0)
                except Exception as e:
                    print(f"âš ï¸ ç¬¬{i}é¡µåŠ è½½å¤±è´¥: {e}")
                    continue

                items = parse_index_items(index_html, base)
                if not items:
                    print(f"âš ï¸ ç¬¬{i}é¡µä¸ºç©ºæˆ–ç»“æ„å˜åŒ–ï¼š{index_url}")
                    continue

                for it in items:
                    if it["url"] in done:
                        continue

                    dpage = await context.new_page()
                    try:
                        detail_html = await fetch_html(dpage, it["url"], "div.classic-block-wrapper, div.entry-content, article", 1.5)
                        content = parse_detail_content(detail_html)
                    except Exception as e:
                        print(f"âš ï¸ è¯¦æƒ…é¡µå¤±è´¥ {it['url']}: {e}")
                        content = ""
                    finally:
                        await dpage.close()

                    row = {
                        "source": base,
                        "type": it["type"],
                        "title": it["title"],
                        "date": it["date"],
                        "url": it["url"],
                        "content": content,
                    }
                    write_dual(row, writer, fh)
                    done.add(it["url"])
                    await asyncio.sleep(1.0)

            await page.close()

        await browser.close()
    fh.close()
    print("ğŸ å…¨éƒ¨å®Œæˆï¼CSV + JSON å·²ä¿å­˜ã€‚")


if __name__ == "__main__":
    asyncio.run(crawl_all())
